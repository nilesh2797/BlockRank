{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# BlockRank Quickstart\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nilesh2797/BlockRank/blob/main/examples/quickstart.ipynb)\n",
    "\n",
    "This notebook demonstrates how to use **BlockRank** for fast, scalable document ranking with LLMs.\n",
    "\n",
    "**BlockRank** makes LLMs efficient for in-context ranking through:\n",
    "- ðŸš€ **Structured Sparse Attention**: Linear complexity (vs quadratic)\n",
    "- âš¡ **Attention-based Inference**: 4.7Ã— faster (no token generation)\n",
    "- ðŸŽ¯ **Auxiliary Contrastive Loss**: Better relevance signals\n",
    "\n",
    "**Paper**: [Scalable In-context Ranking with Generative Models](https://arxiv.org/abs/2510.05396)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install BlockRank and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install BlockRank from GitHub\n",
    "!pip install -q git+https://github.com/nilesh2797/BlockRank.git\n",
    "\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sample data with 2 queries\n"
     ]
    }
   ],
   "source": [
    "# create a simple example\n",
    "import json\n",
    "\n",
    "sample_data = [\n",
    "    {\n",
    "        \"query\": \"what is the capital of france\",\n",
    "        \"query_id\": \"q1\",\n",
    "        \"documents\": [\n",
    "            {\"doc_id\": \"0\", \"title\": \"Paris\", \"text\": \"Paris is the capital and most populous city of France.\"},\n",
    "            {\"doc_id\": \"1\", \"title\": \"Berlin\", \"text\": \"Berlin is the capital and largest city of Germany.\"},\n",
    "            {\"doc_id\": \"2\", \"title\": \"London\", \"text\": \"London is the capital of England and the United Kingdom.\"},\n",
    "            {\"doc_id\": \"3\", \"title\": \"Rome\", \"text\": \"Rome is the capital city of Italy.\"},\n",
    "            {\"doc_id\": \"4\", \"title\": \"Madrid\", \"text\": \"Madrid is the capital and most populous city of Spain.\"},\n",
    "        ],\n",
    "        \"answer_ids\": [\"0\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"who wrote the novel 1984\",\n",
    "        \"query_id\": \"q2\",\n",
    "        \"documents\": [\n",
    "            {\"doc_id\": \"0\", \"title\": \"George Orwell\", \"text\": \"George Orwell wrote the dystopian novel Nineteen Eighty-Four in 1949.\"},\n",
    "            {\"doc_id\": \"1\", \"title\": \"Aldous Huxley\", \"text\": \"Aldous Huxley wrote Brave New World in 1932.\"},\n",
    "            {\"doc_id\": \"2\", \"title\": \"Ray Bradbury\", \"text\": \"Ray Bradbury is the author of Fahrenheit 451.\"},\n",
    "            {\"doc_id\": \"3\", \"title\": \"J.R.R. Tolkien\", \"text\": \"J.R.R. Tolkien wrote The Lord of the Rings.\"},\n",
    "            {\"doc_id\": \"4\", \"title\": \"Ernest Hemingway\", \"text\": \"Ernest Hemingway was an American novelist and journalist.\"},\n",
    "        ],\n",
    "        \"answer_ids\": [\"0\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to file\n",
    "with open(\"sample_data.jsonl\", \"w\") as f:\n",
    "    for item in sample_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Created sample data with {len(sample_data)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained BlockRank Model\n",
    "\n",
    "Load a BlockRank model fine-tuned on MS MARCO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading quicktensor/blockrank-msmarco-mistral-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f247e1919b4416ab8306edaf208524b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from blockrank import blockrank_triton_kernel_attention, blockrank_std_attention\n",
    "blockrank_triton_kernel_attention.register_triton_blockrank_attention()\n",
    "blockrank_std_attention.register_blockrank_attention()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"quicktensor/blockrank-msmarco-mistral-7b\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    quantization_config=quantization_config,\n",
    "    attn_implementation=\"triton_blockrank\"\n",
    ")\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Attention-Based Inference\n",
    "\n",
    "Use BlockRank's fast attention-based inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae1e135c90743b9bab5766f6d1551b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdaafd2fb1c34f53a4b79e1534425ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3b989b183343de99d517c86f87c2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 examples\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Import evaluation utilities\n",
    "from blockrank.dataset import load_icr_dataset_hf, block_icr_collate_fn\n",
    "from blockrank.utils import calculate_accuracy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "# Load dataset\n",
    "ds = load_icr_dataset_hf(\n",
    "    data_path=\"sample_data.jsonl\",\n",
    "    tokenizer=tokenizer,\n",
    "    num_documents=-1,\n",
    "    eval_mode=True,\n",
    "    use_blockrank=True,\n",
    ")\n",
    "\n",
    "eval_ds = ds[\"train\"]\n",
    "print(f\"Loaded {len(eval_ds)} examples\")\n",
    "\n",
    "# Setup data collator\n",
    "data_collator = partial(\n",
    "    block_icr_collate_fn,\n",
    "    tok=tokenizer,\n",
    "    max_block_length=512,\n",
    "    pad_to_multiple_of=16\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n",
      "\n",
      "Query 1: what is the capital of france\n",
      "  Predicted ranking: [4, 3, 1, 2, 0]\n",
      "  Ground truth: [4]\n",
      "\n",
      "Query 2: who wrote the novel 1984\n",
      "  Predicted ranking: [4, 0, 2, 3, 1]\n",
      "  Ground truth: [4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "all_predictions = []\n",
    "attn_layer_idx = 20  # 20th layer for Mistral-7B\n",
    "\n",
    "print(\"Running inference...\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        # Move to device\n",
    "        device = model.device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            **batch,\n",
    "            output_attentions=True,\n",
    "            layers_to_return_scores=[attn_layer_idx],\n",
    "            use_cache=False\n",
    "        )\n",
    "        \n",
    "        # Extract attention scores\n",
    "        attention = outputs.attentions[0]\n",
    "        B, M, H = batch['attention_mask'].shape\n",
    "        _, N, H1, MH = attention.shape\n",
    "        \n",
    "        # Compute document scores from attention\n",
    "        attn = F.softmax(attention[:, :, -1, H:-H], dim=-1)\n",
    "        attn = attn.reshape(B, N, -1, H)\n",
    "        attn_scores = attn.mean(1).sum(-1)\n",
    "        \n",
    "        # Get top-k predictions\n",
    "        k = min(5, attn_scores.shape[-1])\n",
    "        top_k_indices = torch.topk(attn_scores, k=k, dim=-1).indices[0].cpu().tolist()\n",
    "        \n",
    "        all_predictions.append(top_k_indices)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"Query {idx + 1}: {eval_ds[idx]['query']}\")\n",
    "        print(f\"  Predicted ranking: {top_k_indices}\")\n",
    "        print(f\"  Ground truth: {eval_ds[idx]['answer_ids'].tolist()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results\n",
    "\n",
    "Calculate ranking metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Evaluation Results\n",
      "============================================================\n",
      "Accuracy (top-1): 100.00%\n",
      "Exact matches: 2 / 2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "results = calculate_accuracy(all_predictions, eval_ds)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Evaluation Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy (top-1): {results['accuracy']:.2f}%\")\n",
    "print(f\"Exact matches: {results['exact_match']} / {results['total']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "- **Paper**: [arXiv:2510.05396](https://arxiv.org/abs/2510.05396)\n",
    "- **GitHub**: [nilesh2797/BlockRank](https://github.com/nilesh2797/BlockRank)\n",
    "- **Training Guide**: See `docs/TRAINING.md`\n",
    "- **Data Format**: See `docs/DATA_FORMAT.md`\n",
    "\n",
    "## Train Your Own Model\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/nilesh2797/BlockRank.git\n",
    "cd BlockRank\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Train on your data\n",
    "python scripts/train.py --config configs/your-config.yaml\n",
    "```\n",
    "\n",
    "## Evaluate on BEIR Benchmarks\n",
    "```bash\n",
    "# Evaluate on single beir dataset\n",
    "python scripts/eval_attn.py \\\n",
    "    --config configs/eval_beir.yaml \\\n",
    "    --checkpoint outputs/your-model\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
