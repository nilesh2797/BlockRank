# Configuration for large-scale training on full dataset

model:
  model_name_or_path: "mistralai/Mistral-7B-Instruct-v0.3"
  use_4bit: false
  use_lora: false
  trust_remote_code: false
  use_blockrank: true
  attn_implementation: "default_blockrank" 

data:
  data_path: "data/blockrank-msmarco-mistral-7b/parsed_hard_ids_10p_train.jsonl"  # Full dataset
  streaming: false
  val_data_path: null  # Separate validation
  num_documents: 30
  train_test_split: 0.99  # Ignored when val_data_path is set
  max_seq_length: 6144
  dataset_seed: 42
  max_block_length: 160

training:
  output_dir: "outputs/full-blockrank-10p-msmarco-mistral-7b"
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4  # Larger effective batch size
  optim: "adamw_8bit"
  learning_rate: 3.0e-6  # Slightly lower for larger dataset
  weight_decay: 0
  warmup_ratio: 0.01
  lr_scheduler_type: "cosine"
  logging_steps: 25
  save_steps: 500
  eval_steps: 500
  save_total_limit: 1
  fp16: false
  bf16: true
  gradient_checkpointing: true
  seed: 42
  report_to: "wandb"
  run_name: "full-blockrank-10p-msmarco-mistral-7b"
  do_eval: true
  evaluation_strategy: "steps"
  load_best_model_at_end: false
  metric_for_best_model: "eval_loss"
  use_aux_loss: true        # Enable auxiliary attention loss
  aux_layer_idx: 20         # Layer to extract attention scores (l* in paper)
  aux_loss_weight: 0.1      # Lambda in paper (default: 0.1)
  aux_temperature: 0.05      # Tau in InfoNCE loss (default: 0.1)